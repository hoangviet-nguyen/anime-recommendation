{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Building a Recommender System with Scikit-learn\n",
    "This notebook focuses on implementing a custom recommender system. The primary objective is to build a generalized model capable of recommending items based on the training data provided. We will leverage the scikit-learn API to compare and evaluate different models to determine the most effective approach for our recommendation task. For more detailed information on using the scikit-learn API, visit the [scikit-learn developer guide.](https://scikit-learn.org/stable/developers/develop.html)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy.sparse import random"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Implementation recommender system\n",
    "\n",
    "### Scikit-Learn API\n",
    "\n",
    "The [Scikit-Learn User Manual](https://scikit-learn.org/stable/developers/develop.html) provides detailed guidelines on how to construct a basic estimator. To ensure compatibility with Scikit-Learn and its suite of estimators, we adhere strictly to these guidelines as outlined in the documentation.\n",
    "\n",
    "\n",
    "### Cost Function Used\n",
    "\n",
    "\\begin{align}\n",
    "J(U, M, \\mathbf{b_u}, \\mathbf{b_m}) &= \\frac{1}{| \\mathbb{Z} |} \\Bigg(\\sum_{u, i \\in \\mathbb{Z}} \\frac{1}{2}(r^{(u,i)} - \\hat{r}^{(u,i)})^2 + \\sum_{u, i \\in \\mathbb{Z}} \\Omega^{(u, i)} \\Bigg)\n",
    "\\end{align}\n",
    "\n",
    "\\begin{align}\n",
    "\\hat{r}^{(u,i)} &= \\mathbf{u}^{(u)T} \\mathbf{m}^{(i)} + b_u^{(u)} + b_m^{(i)} \\\\\n",
    "\\Omega^{(u, i)} &= \\frac{\\lambda}{2} \\Bigg( \\lVert \\mathbf{u}^{(u)} \\rVert^2_2 +  \\lVert \\mathbf{m}^{(i)} \\rVert^2_2 + b_u^{(u)2} + b_m^{(i)2} \\Bigg)\n",
    "\\end{align}\n",
    "\n",
    "Here, \\( U \\) represents the users, and \\( M \\) represents the items. The cost function is only applied to the entries in \\( r \\) where a rating exists.\n",
    "\n",
    "$\\mathbb{Z}$ is the set of known/non-zero entries in the rating matrix \\( R \\) (i.e., all combinations of \\( (u, i) \\) that occur). $\\mathbf{u}^{(u)T}$ is the \\( u \\)-th row vector of \\( U \\), and $\\mathbf{m}^{(i)}$ is the \\( i \\)-th column vector of $\\mathbf{M}^{T}$. $\\lambda$ is the regularization factor. The global average rating \\( r \\) is computed on \\( R \\), and then subtracted from all ratings. Therefore, the optimization is performed on $\\R_{\\text{center}}$ = R - r. The squared \\( L_2 \\) norm is defined as: $\\|z\\|_2^2 = \\sum_i (z(i))^2$.\n",
    "\n",
    "\n",
    "### Optimization with Stochastic Gradient Descent (SGD)\n",
    "The cost function is optimized alternately, first for \\( \\mathbf{u}^{(u)T} \\) and then for \\( \\mathbf{m}^{(i)} \\) (and the biases), while keeping the other variable constant. This leads to a least squares optimization problem. With SGD, the cost function (and its gradients) is approximated using a per-sample loss \\( L \\):\n",
    "\n",
    "With \\( M \\) fixed:\n",
    "\\begin{align}\n",
    "L(U, r^{(u,i)}) &= \\frac{1}{2} (r^{(u,i)} - \\hat{r}^{(u,i)})^2 + \\Omega^{(u, i)} \\\\\n",
    "J(U) &= \\mathbb{E}_{u,i \\sim \\mathbb{Z}} L(U, r^{(u,i)}) = \\frac{1}{| \\mathbb{Z} |} \\sum_{u, i \\in \\mathbb{Z}}  L(U, r^{(u,i)})\n",
    "\\end{align}\n",
    "\n",
    "With \\( U \\) fixed:\n",
    "\\begin{align}\n",
    "L(M, r^{(u,i)}) &= \\frac{1}{2} (r^{(u,i)} - \\hat{r}^{(u,i)})^2 + \\Omega^{(u, i)} \\\\\n",
    "J(M) &= \\mathbb{E}_{u,i \\sim \\mathbb{Z}} L(M, r^{(u,i)}) = \\frac{1}{| \\mathbb{Z} |} \\sum_{u, i \\in \\mathbb{Z}}  L(M, r^{(u,i)})\n",
    "\\end{align}\n",
    "\n",
    "### Algorithm\n",
    "We optimize the cost function for each entry in the rating matrix \\( R \\) using the following algorithm, where \\( \\eta \\) is the learning rate and \\( t \\) indexes the iteration step.\n",
    "\n",
    "For all \\( u,i \\in \\mathbb{Z} \\) do:\n",
    "\\begin{align}\n",
    "\\mathbf{u}^{(u)}_{t+1} &= \\mathbf{u}^{(u)}_{t} - \\eta \\frac{\\partial L(U, r^{(u,i)})}{\\partial \\mathbf{u}^{(u)}_{t}} \\\\\n",
    "\\mathbf{m}^{(i)}_{t+1} &= \\mathbf{m}^{(i)}_{t} - \\eta \\frac{\\partial L(M, r^{(u,i)})}{\\partial \\mathbf{m}^{(i)}_{t}} \\\\\n",
    "b_{m, t+1}^{(i)} &= b_{m, t}^{(i)} - \\eta \\frac{\\partial L(M, r^{(u,i)})}{\\partial b_{m, t}^{(i)}} \\\\\n",
    "b_{u, t+1}^{(u)} &= b_{u, t}^{(u)} - \\eta \\frac{\\partial L(U, r^{(u,i)})}{\\partial b_{u, t}^{(u)}} \\\\\n",
    "\\end{align}\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.base import BaseEstimator\n",
    "import numpy as np\n",
    "from tqdm.notebook import tqdm\n",
    "\n",
    "class RecommenderModel(BaseEstimator):\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        alpha: float=0, \n",
    "        lr:float=0.1,\n",
    "        num_components: int=10,\n",
    "        num_epochs: int=10000,\n",
    "        random_seed: int=12345,\n",
    "        batch_size = 10,\n",
    "        epsilon=1e-10\n",
    "    ):\n",
    "        \n",
    "        \"\"\"\n",
    "        Args:\n",
    "            alpha: regularization constant\n",
    "            lr: learning rate constant\n",
    "            num_components: dimensionaliy of each user / item vector\n",
    "            num_epochs: number of passes over each entry in the Rating matrix\n",
    "            random_seed: random seed for reproducibility\n",
    "        \"\"\"\n",
    "\n",
    "        self.alpha = alpha\n",
    "        self.lr = lr\n",
    "        self.num_components = num_components\n",
    "        self.num_epochs = num_epochs\n",
    "        self.random_seed = random_seed\n",
    "        self.batch_size = batch_size\n",
    "        self.epsilon = epsilon\n",
    "    \n",
    "\n",
    "    def __init_variables__(self, X:np.ndarray) -> None:\n",
    "        \n",
    "        self.default_rng = np.random.default_rng(self.random_seed)\n",
    "        self.num_user, self.num_items = X.shape\n",
    "        self.user_matrix = self.default_rng.normal(0, 0.1, (self.num_user, self.num_components))\n",
    "        self.item_matrix = self.default_rng.normal(0, 1, (self.num_items, self.num_components))\n",
    "        self.item_bias = np.zeros((self.num_items, 1))\n",
    "        self.user_bias = np.zeros((self.num_user, 1))\n",
    "        self.rating = self.bool_matrix(X)\n",
    "        self.num_ratings = np.sum(self.rating)\n",
    "        self.global_avg = np.sum(X) / (self.num_ratings if self.num_ratings > 0 else 1)\n",
    "\n",
    "        # for plotting\n",
    "        self.learning_curve = []\n",
    "        self.cost_curve = []\n",
    "        self.stop_step = 0\n",
    "\n",
    "   \n",
    "    def fit(self, X: np.ndarray):\n",
    "        \"\"\"Fit the model.\n",
    "        \n",
    "            Args:\n",
    "                X (num_users, num_items) - The Rating Matrix\n",
    "    \n",
    "            Returns:\n",
    "                Self\n",
    "        \"\"\"\n",
    "        self.__init_variables__(X)\n",
    "        user_indices, item_indices = np.where(self.rating != 0)\n",
    "        indices = np.arange(len(user_indices))\n",
    "        for epoch in range(self.num_epochs):\n",
    "            prediction = self.predict()\n",
    "            self.cost_curve.append(self.calc_cost(X, prediction))\n",
    "            np.random.shuffle(indices)\n",
    "            for index in range(len(indices)):\n",
    "                user = user_indices[index]\n",
    "                item = item_indices[index]\n",
    "\n",
    "                actual_rating = X[user, item]\n",
    "                prediction = np.dot(self.user_matrix[user].T, self.item_matrix[item]) + self.item_bias[item] +  self.user_bias[user]\n",
    "                error = actual_rating - prediction\n",
    "    \n",
    "                dj_du, dj_dm, dj_dbu, dj_dbi = self.calc_gradients(error=error, m_item=item, u_user=user)\n",
    "\n",
    "                self.user_matrix[user] -= self.lr * dj_du\n",
    "                self.item_matrix[item] -= self.lr * dj_dm\n",
    "                self.user_bias[user] -= self.lr * dj_dbu\n",
    "                self.item_bias[item] -= self.lr * dj_dbi\n",
    "\n",
    "                gradient_norm = (\n",
    "                    np.linalg.norm(dj_du) + \n",
    "                    np.linalg.norm(dj_dm) + \n",
    "                    np.linalg.norm(dj_dbu) + \n",
    "                    np.linalg.norm(dj_dbi)\n",
    "                )\n",
    "                \n",
    "                self.learning_curve.append(gradient_norm)\n",
    "\n",
    "                if gradient_norm < self.epsilon:\n",
    "                    self.stop_step = epoch\n",
    "                    self.last_gradient = self.learning_curve[-1]\n",
    "                    return self\n",
    "\n",
    "        return self\n",
    "    \n",
    "    def print_eps_condition(self):\n",
    "        if(self.stop_step != 0):\n",
    "            print(f\"The epsilon criterion was met in step: {self.stop_step}\")\n",
    "            print(f\"The value of the last gradient is: {self.last_gradient}\")\n",
    "        else: \n",
    "            print(\"The stopping criterion was not met\")\n",
    "            print(f\"The value of the last gradient is: {self.learning_curve[-1]}\")\n",
    "    \n",
    "    def calc_cost(self, y_true: np.ndarray, y_pred: np.ndarray) -> float:\n",
    "        \"\"\" Cost function\n",
    "            Args:\n",
    "                y_true: Ratings Matrix [num_users, num_items]\n",
    "                y_pred: Predicted Ratings Matrix [num_users, num_items]\n",
    "            Returns:\n",
    "                cost for a predicion\n",
    "        \"\"\"\n",
    "\n",
    "        omega = self.alpha / 2 * (np.sum(self.user_matrix**2) + np.sum(self.item_matrix**2) + np.sum(self.item_bias**2) + np.sum(self.user_bias**2))\n",
    "        residual = y_true - y_pred * self.rating \n",
    "        cost = (np.sum(residual**2) / 2 + omega) / self.num_ratings\n",
    "        return cost\n",
    "    \n",
    "    def calc_gradients(\n",
    "            self,\n",
    "            error: np.ndarray,\n",
    "            m_item: np.ndarray,\n",
    "            u_user: np.ndarray) -> (np.ndarray, np.ndarray, np.ndarray, np.ndarray):\n",
    "        \"\"\" Calculate Gradients\n",
    "            Args:\n",
    "                error: Matrix with size [batch_size, batch_size]\n",
    "                m_item: weights for chosen items [batch_size, num_components]\n",
    "                u_user: weights for chosen users [batch_size, num_components]\n",
    "                bias_user: bias for chosen user [batch_size]\n",
    "                bias_item: bias for chosen item [batch_size]\n",
    "            Returns:\n",
    "                Tuple (dJ_dUser, dJ_dItem, dJ_dUserBias, dJ_dItemBias)\n",
    "                \n",
    "                with shapes:\n",
    "                \n",
    "                ([batch_size, num_components], [batch_size, num_components], [batch_size], [batch_size])\n",
    "        \"\"\"\n",
    "\n",
    "        user, item = self.user_matrix[u_user], self.item_matrix[m_item]\n",
    "        dj_du = np.multiply(error, -item) + self.alpha * user\n",
    "        dj_dm = np.multiply(error, -user) + self.alpha * item\n",
    "\n",
    "        dj_dbu = -error + self.alpha * self.user_bias[u_user]\n",
    "        dj_dbi = -error + self.alpha * self.item_bias[m_item]\n",
    "        return dj_du, dj_dm, dj_dbu, dj_dbi\n",
    "\n",
    "\n",
    "\n",
    "    def predict(self) -> np.ndarray:\n",
    "        \"\"\" Calculate Approximation of Rating Matrix\n",
    "            Returns:\n",
    "                Approximation of Rating Matrix [num_users, num_items]\n",
    "        \"\"\"\n",
    "        prediction = np.matmul(self.user_matrix, self.item_matrix.T)\n",
    "        prediction += self.user_bias\n",
    "        prediction += self.item_bias.T\n",
    "        return prediction\n",
    "    \n",
    "    def score(self, X: np.ndarray) -> float:\n",
    "        \"\"\" Mean Squared Error between X and Approximation\n",
    "            Args:\n",
    "                X: Ratings Matrix [num_users, num_items]\n",
    "            Returns:\n",
    "                MSE (float)\n",
    "        \"\"\"\n",
    "        rating = self.bool_matrix(X)\n",
    "        error = ((self.predict() - X) * rating)**2\n",
    "        score = np.sum(error) / np.sum(rating)\n",
    "        return score\n",
    "\n",
    "    def score_mae(self, X: np.ndarray) -> float:\n",
    "        \"\"\" Mean Absolute Error between X and Approximation\n",
    "            Args:\n",
    "                X: Ratings Matrix [num_users, num_items]\n",
    "            Returns:\n",
    "                MAE (float)\n",
    "        \"\"\"\n",
    "        rating = self.bool_matrix(X)\n",
    "        error = np.abs(((self.predict() - X) * rating))\n",
    "        score = np.sum(error) / np.sum(rating)\n",
    "        return score\n",
    "    \n",
    "\n",
    "    def bool_matrix(self, X :np.ndarray) -> np.ndarray:\n",
    "        '''\n",
    "        Creates a boolean matrix 1 if the given argument has a rating else 0\n",
    "        Args:\n",
    "            X: Rating matrix \n",
    "        '''\n",
    "        rating_matrix = np.where(X != 0, 1, 0)\n",
    "        return rating_matrix\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Testing the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Passed test: test_ones_matrix\n",
      "----> Expected: [[1. 1. 1. 1. 1.]\n",
      " [1. 1. 1. 1. 1.]\n",
      " [1. 1. 1. 1. 1.]\n",
      " [1. 1. 1. 1. 1.]\n",
      " [1. 1. 1. 1. 1.]]\n",
      "----> Actual: [[1.04241708 0.99754131 0.97477828 0.98976791 0.98844079]\n",
      " [0.96051725 1.01274295 1.0034863  1.0273135  0.93996663]\n",
      " [0.95719652 1.01027234 1.0099667  1.02264772 0.95703827]\n",
      " [1.00096004 0.99983107 0.9982038  0.99805037 0.99629228]\n",
      " [1.02090523 1.00228524 0.97943205 1.00156645 0.96745133]]\n",
      "Passed test: test_twos_matrix\n",
      "----> Expected: [[2. 2. 2. 2. 2.]\n",
      " [2. 2. 2. 2. 2.]\n",
      " [2. 2. 2. 2. 2.]\n",
      " [2. 2. 2. 2. 2.]\n",
      " [2. 2. 2. 2. 2.]]\n",
      "----> Actual: [[2.04227915 2.00084606 1.97277732 1.99453585 1.97593792]\n",
      " [1.96185144 2.01639516 2.00302742 2.03263752 1.93195392]\n",
      " [1.95763157 2.01397925 2.00864229 2.02842542 1.94594784]\n",
      " [1.99990123 2.00354796 1.99512691 2.00418237 1.97948204]\n",
      " [2.02058948 2.00663573 1.97716368 2.00838941 1.95199586]]\n",
      "Passed test: test_random_matrix\n",
      "----> Expected: [[4 1 4 2]\n",
      " [3 5 2 2]\n",
      " [2 3 1 4]\n",
      " [2 3 2 2]]\n",
      "----> Actual: [[3.99999615 0.99999965 4.00000099 1.99999988]\n",
      " [2.99999564 4.99999959 2.00000123 1.99999986]\n",
      " [1.99999407 2.99999945 1.00000166 3.99999981]\n",
      " [2.00000887 3.00000073 1.99999827 2.00000027]]\n",
      "Passed test: test_sparse_matrix\n",
      "----> Expected: [5 3 1 2 4 3 2 1 5 3 4 2 4 1 4 2 5]\n",
      "----> Actual: [5. 3. 1. 2. 4. 3. 2. 1. 5. 3. 4. 2. 4. 1. 4. 2. 5.]\n",
      "Passed test: test_sparse_matrix\n",
      "----> Expected: all larger than 0.0\n",
      "----> Actual: [1.62954818 2.52139131 1.57685841 0.47958919 2.17606369 0.86488698\n",
      " 1.92444215 3.13753578 1.87078141 4.03140278 2.29041291 3.64869139\n",
      " 2.32135659]\n",
      "Passed test: test_random_sparse_matrix\n",
      "----> Expected: [3 4 2 5 1]\n",
      "----> Actual: [2.99999731 3.94958869 2.01841294 5.         1.05911813]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "def print_result(test_name, passed, expected, actual):\n",
    "    status = \"Passed\" if passed else \"Failed\"\n",
    "    print(f\"{status} test: {test_name}\")\n",
    "    print(f\"----> Expected: {expected}\")\n",
    "    print(f\"----> Actual: {actual}\")\n",
    "\n",
    "def test_ones_matrix():\n",
    "    # Create a ones matrix\n",
    "    X = np.ones((5, 5))\n",
    "    \n",
    "    # Initialize the model\n",
    "    model = RecommenderModel(alpha=0, lr=0.01, num_components=2, num_epochs=50, random_seed=123)\n",
    "    \n",
    "    # Fit the model\n",
    "    model.fit(X)\n",
    "    \n",
    "    # Make predictions\n",
    "    predictions = model.predict()\n",
    "    \n",
    "    try:\n",
    "        np.testing.assert_allclose(predictions, 1, atol=0.1)\n",
    "        print_result(\"test_ones_matrix\", True, np.ones((5, 5)), predictions)\n",
    "    except AssertionError as e:\n",
    "        print_result(\"test_ones_matrix\", False, np.ones((5, 5)), predictions)\n",
    "\n",
    "\n",
    "def test_twos_matrix():\n",
    "    X = np.ones((5, 5)) + 1\n",
    "    \n",
    "    # Initialize the model\n",
    "    model = RecommenderModel(alpha=0, lr=0.01, num_components=2, num_epochs=50, random_seed=123)\n",
    "    \n",
    "    # Fit the model\n",
    "    model.fit(X)\n",
    "    \n",
    "    # Make predictions\n",
    "    predictions = model.predict()\n",
    "    \n",
    "    try:\n",
    "        np.testing.assert_allclose(predictions, 2, atol=0.1)\n",
    "        print_result(\"test_twos_matrix\", True, np.ones((5, 5)) + 1, predictions)\n",
    "    except AssertionError as e:\n",
    "        print_result(\"test_twos_matrix\", False, np.ones((5, 5)) + 1, predictions)\n",
    "\n",
    "\n",
    "def test_random_matrix():\n",
    "    X = np.array([\n",
    "        [4, 1, 4, 2],\n",
    "        [3, 5, 2, 2],\n",
    "        [2, 3, 1, 4],\n",
    "        [2, 3, 2, 2]\n",
    "    ])\n",
    "    \n",
    "    # Initialize the model\n",
    "    model = RecommenderModel(alpha=0, lr=0.1, num_components=4, num_epochs=100, random_seed=123)\n",
    "    \n",
    "    # Fit the model\n",
    "    model.fit(X)\n",
    "    \n",
    "    # Make predictions\n",
    "    predictions = model.predict()\n",
    "    \n",
    "    # The predictions should be close to the input, since there are no missing (zero) entries\n",
    "    try:\n",
    "        np.testing.assert_allclose(predictions, X, atol=0.1)\n",
    "        print_result(\"test_random_matrix\", True, X, predictions)\n",
    "    except AssertionError as e:\n",
    "        print_result(\"test_random_matrix\", False, X, predictions)\n",
    "\n",
    "        \n",
    "def test_sparse_matrix():\n",
    "    # Create a 5x6 matrix with random values and some zeros\n",
    "    X = np.array([\n",
    "        [5, 3, 0, 1, 2, 0],\n",
    "        [4, 0, 3, 0, 0, 2],\n",
    "        [0, 1, 5, 0, 3, 4],\n",
    "        [2, 0, 0, 4, 1, 0],\n",
    "        [0, 4, 2, 0, 0, 5]\n",
    "    ])\n",
    "    \n",
    "    # Initialize the model\n",
    "    model = RecommenderModel(alpha=0, lr=0.1, num_components=4, num_epochs=100, random_seed=123)\n",
    "    \n",
    "    # Fit the model\n",
    "    model.fit(X)\n",
    "    \n",
    "    # Make predictions\n",
    "    predictions = model.predict()\n",
    "    \n",
    "    try:\n",
    "        # Check that non-zero entries are close to the input values\n",
    "        non_zero_indices = np.nonzero(X)\n",
    "        np.testing.assert_allclose(predictions[non_zero_indices], X[non_zero_indices], atol=1.0)\n",
    "        print_result(\"test_sparse_matrix\", True, X[non_zero_indices], predictions[non_zero_indices])\n",
    "    except:\n",
    "        print_result(\"test_sparse_matrix\", False, X[non_zero_indices], predictions[non_zero_indices])\n",
    "    \n",
    "    try:\n",
    "        # Check that zero entries in the original matrix are now non-zero in the predictions\n",
    "        zero_indices = np.where(X == 0)\n",
    "        assert np.all(predictions[zero_indices] >= 0.0)\n",
    "        print_result(\"test_sparse_matrix\", True, \"all larger than 0.0\" , predictions[zero_indices])\n",
    "    except AssertionError as e:\n",
    "        print_result(\"test_sparse_matrix\", False, \"all larger than 0.0\", predictions[zero_indices])\n",
    "\n",
    "\n",
    "def test_random_sparse_matrix():\n",
    "    # Create a random sparse matrix with more zeros\n",
    "    X = np.array([\n",
    "        [0, 0, 3, 0],\n",
    "        [4, 0, 0, 2],\n",
    "        [0, 5, 0, 0],\n",
    "        [1, 0, 0, 0]\n",
    "    ])\n",
    "    \n",
    "    # Initialize the model\n",
    "    model = RecommenderModel(alpha=0, lr=0.1, num_components=2, num_epochs=100, random_seed=123)\n",
    "    \n",
    "    # Fit the model\n",
    "    model.fit(X)\n",
    "    \n",
    "    # Make predictions\n",
    "    predictions = model.predict()\n",
    "    \n",
    "    # The non-zero entries should be close to the input values\n",
    "    try:\n",
    "        non_zero_indices = np.nonzero(X)\n",
    "        np.testing.assert_allclose(predictions[non_zero_indices], X[non_zero_indices], atol=1.0)\n",
    "        print_result(\"test_random_sparse_matrix\", True, X[non_zero_indices], predictions[non_zero_indices])\n",
    "    except AssertionError as e:\n",
    "        print_result(\"test_random_sparse_matrix\", False, X[non_zero_indices], predictions[non_zero_indices])\n",
    "\n",
    "for test in [\n",
    "    test_ones_matrix, \n",
    "    test_twos_matrix, \n",
    "    test_random_matrix, \n",
    "    test_sparse_matrix, \n",
    "    test_random_sparse_matrix, \n",
    "]:\n",
    "    try:\n",
    "        test()\n",
    "    except Exception as e:\n",
    "        print(f\"Error during testing - test: {test} error: {e}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
